{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1728879,"sourceType":"datasetVersion","datasetId":1025978}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft accelerate torch\n!pip install huggingface_hub[hf_xet]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:27:39.926819Z","iopub.status.idle":"2025-04-24T10:27:39.927572Z","shell.execute_reply.started":"2025-04-24T10:27:39.927378Z","shell.execute_reply":"2025-04-24T10:27:39.927398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\nimport gc\n\nclass RecipeDataProcessor:\n    def __init__(self, sample_size=100):\n        self.sample_size = sample_size\n        self.tokenizer = None\n        self.dataset = None\n        \n    def load_data(self, file_path):\n        \"\"\"Load and preprocess the RecipeNLG dataset\"\"\"\n        try:\n            # Load only necessary columns with explicit dtype to reduce memory\n            df = pd.read_csv(\n                file_path,\n                usecols=['title', 'ingredients', 'directions', 'NER'],\n                dtype={\n                    'title': 'string',\n                    'ingredients': 'string',\n                    'directions': 'string',\n                    'NER': 'string'\n                }\n            )\n            \n            # Sample the dataset (while maintaining reproducibility)\n            df = df.sample(n=self.sample_size, random_state=42).reset_index(drop=True)\n            \n            # Clean and format the text\n            df['text'] = self._format_recipes(df)\n            \n            # Convert to HuggingFace Dataset\n            self.dataset = Dataset.from_pandas(df[['text']])\n            \n            # Clean up\n            del df\n            gc.collect()\n            \n            print(f\"Successfully loaded and processed {self.sample_size} samples.\")\n            return True\n            \n        except Exception as e:\n            print(f\"Error loading dataset: {str(e)}\")\n            return False\n    \n    def _format_recipes(self, df):\n        \"\"\"Format recipe data into a consistent text format\"\"\"\n        formatted_texts = []\n        \n        for _, row in df.iterrows():\n            # Basic cleaning\n            title = str(row['title']).strip()\n            ingredients = str(row['ingredients']).strip().replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n            directions = str(row['directions']).strip().replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n            ner = str(row['NER']).strip().replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n            \n            # Create structured prompt\n            formatted_text = (\n                f\"Recipe Title: {title}\\n\\n\"\n                f\"Ingredients:\\n{ingredients}\\n\\n\"\n                f\"Instructions:\\n{directions}\\n\\n\"\n                f\"Food Entities: {ner}\"\n            )\n            formatted_texts.append(formatted_text)\n            \n        return formatted_texts\n    \n    def initialize_tokenizer(self, model_name=\"gpt2\"):\n        \"\"\"Initialize the tokenizer\"\"\"\n        try:\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token\n            print(f\"Initialized tokenizer for model: {model_name}\")\n            return True\n        except Exception as e:\n            print(f\"Error initializing tokenizer: {str(e)}\")\n            return False\n    \n    def split_dataset(self, test_size=0.1, seed=42):\n        \"\"\"Split the dataset into training and test sets\"\"\"\n        if not hasattr(self, 'dataset') or self.dataset is None:\n            print(\"No dataset loaded to split!\")\n            return False\n            \n        try:\n            # Perform the split\n            split_dataset = self.dataset.train_test_split(\n                test_size=test_size,\n                seed=seed\n            )\n            \n            # Convert to DatasetDict for proper HuggingFace handling\n            self.dataset = DatasetDict({\n                'train': split_dataset['train'],\n                'test': split_dataset['test']\n            })\n            \n            print(f\"Successfully split dataset: {len(self.dataset['train'])} train, {len(self.dataset['test'])} test samples\")\n            return True\n            \n        except Exception as e:\n            print(f\"Error splitting dataset: {str(e)}\")\n            return False\n    \n    def tokenize_dataset(self, max_length=512):\n        \"\"\"Tokenize the dataset\"\"\"\n        if not self.tokenizer:\n            print(\"Tokenizer not initialized!\")\n            return False\n            \n        def tokenize_function(examples):\n            return self.tokenizer(\n                examples[\"text\"],\n                truncation=True,\n                padding=\"max_length\",\n                max_length=max_length,\n                return_tensors=\"pt\"\n            )\n            \n        try:\n            self.dataset = self.dataset.map(\n                tokenize_function,\n                batched=True,\n                remove_columns=[\"text\"]  # Remove original text column\n            )\n            print(\"Successfully tokenized dataset.\")\n            return True\n        except Exception as e:\n            print(f\"Error tokenizing dataset: {str(e)}\")\n            return False\n\n# Correct Usage Example\nif __name__ == \"__main__\":\n    # Initialize processor\n    processor = RecipeDataProcessor(sample_size=100)\n    \n    # 1. Load and process data\n    processor.load_data(\"/kaggle/input/recipenlg/RecipeNLG_dataset.csv\")\n    \n    # 2. Initialize tokenizer\n    processor.initialize_tokenizer(\"gpt2\")\n    \n    # 3. Split dataset (now this method exists!)\n    processor.split_dataset(test_size=0.1)  # 90% train, 10% test\n    \n    # 4. Tokenize dataset\n    processor.tokenize_dataset(max_length=256)\n    \n    # Now you can access:\n    # - processor.dataset['train'] for training\n    # - processor.dataset['test'] for evaluation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:08:21.342924Z","iopub.execute_input":"2025-04-24T11:08:21.343531Z","iopub.status.idle":"2025-04-24T11:08:47.557116Z","shell.execute_reply.started":"2025-04-24T11:08:21.343506Z","shell.execute_reply":"2025-04-24T11:08:47.556374Z"}},"outputs":[{"name":"stdout","text":"Successfully loaded and processed 100 samples.\nInitialized tokenizer for model: gpt2\nSuccessfully split dataset: 90 train, 10 test samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/90 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52acf35f76c940888e01996baa1a6220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec58549de68472187105d40977d404f"}},"metadata":{}},{"name":"stdout","text":"Successfully tokenized dataset.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nclass RecipeModel:\n    def __init__(self, model_name=\"gpt2-medium\"):\n        self.model_name = model_name\n        self.model = None\n        self.tokenizer = None\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n    def initialize_model_and_tokenizer(self):\n        \"\"\"Initialize both model and tokenizer\"\"\"\n        try:\n            # Initialize tokenizer with special tokens for recipes\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Load model with adjusted configuration\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                pad_token_id=self.tokenizer.eos_token_id\n            ).to(self.device)\n            \n            # Resize token embeddings if needed\n            self.model.resize_token_embeddings(len(self.tokenizer))\n            \n            print(f\"Successfully initialized {self.model_name} on {self.device}\")\n            return True\n            \n        except Exception as e:\n            print(f\"Error initializing model: {str(e)}\")\n            return False\n    \n    def generate_recipe(self, prompt, **generation_kwargs):\n        \"\"\"Generate recipe text from prompt\"\"\"\n        if not self.model or not self.tokenizer:\n            print(\"Model or tokenizer not initialized!\")\n            return None\n            \n        try:\n            # Default generation parameters (can be overridden)\n            default_params = {\n                'max_length': 300,\n                'num_return_sequences': 1,\n                'temperature': 0.7,\n                'top_k': 50,\n                'top_p': 0.9,\n                'do_sample': True,\n                'no_repeat_ngram_size': 2,\n                'early_stopping': True\n            }\n            \n            # Update with any provided kwargs\n            generation_params = {**default_params, **generation_kwargs}\n            \n            # Tokenize input\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n            \n            # Generate output\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    **generation_params\n                )\n            \n            # Decode and clean output\n            generated_text = self.tokenizer.decode(\n                outputs[0], \n                skip_special_tokens=True\n            )\n            \n            # Post-processing\n            generated_text = self._post_process(generated_text)\n            return generated_text\n            \n        except Exception as e:\n            print(f\"Error during generation: {str(e)}\")\n            return None\n    \n    def _post_process(self, text):\n        \"\"\"Clean up generated recipe text\"\"\"\n        # Remove any incomplete sentences at the end\n        last_period = text.rfind('.')\n        if last_period != -1:\n            text = text[:last_period+1]\n            \n        # Ensure proper section formatting\n        sections = [\"Recipe Title:\", \"Ingredients:\", \"Instructions:\"]\n        for section in sections:\n            if section not in text:\n                text = text.replace(section.lower(), section)\n                \n        return text.strip()\n\n# Example Usage\nif __name__ == \"__main__\":\n    # Initialize model handler\n    recipe_model = RecipeModel(\"gpt2-medium\")\n    \n    # Set up model and tokenizer\n    if recipe_model.initialize_model_and_tokenizer():\n        # Example prompt\n        prompt = \"Generate a chocolate chip cookie recipe with these ingredients: flour, sugar, eggs, chocolate chips\\n\\nRecipe Title:\"\n        \n        # Generate recipe\n        generated_recipe = recipe_model.generate_recipe(\n            prompt,\n            max_length=400,  # Override default\n            temperature=0.8  # More creative\n        )\n        \n        print(\"\\nGenerated Recipe:\\n\")\n        print(generated_recipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T11:09:00.770583Z","iopub.execute_input":"2025-04-24T11:09:00.770824Z","iopub.status.idle":"2025-04-24T11:09:47.470325Z","shell.execute_reply.started":"2025-04-24T11:09:00.770808Z","shell.execute_reply":"2025-04-24T11:09:47.469640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"218f1e9a670743acb462c90b4caf34ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e61dfed6f364b05a9f2960d98c90f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a06c06b1b7554756bc59d2768dd6c041"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0e02e9503849fd8f488bde84cc6ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d63ecf39a279489380d9f33720c7a00d"}},"metadata":{}},{"name":"stderr","text":"2025-04-24 11:09:13.130825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745492953.637773      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745492953.787049      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ff8f4eda6144fea3852ba51c6e7122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28bd5ee2a87c41528ca97244ad34e50f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Successfully initialized gpt2-medium on cuda\n\nGenerated Recipe:\n\nGenerate a chocolate chip cookie recipe with these ingredients: flour, sugar, eggs, chocolate chips\n\nRecipe Title: Chocolate Chip Cookie Recipe Ingredients: For the dough:\n and 1 cup butter, melted, divided\n, 3/4 cup sugar and 4 eggs\n the rest of the butter melted\nfor the filling: 4 cups all purpose flour\n5/8 cup semi-sweet chocolate, softened\n1 1/2 cups sugar\n3/16 tsp salt\n4 1 1 / 2 cups flour For baking: and 3 1‚ÅÑ 2 tsp baking soda\n2 1 tsp vanilla extract\nFor the baking tray: 2 large (18-inch) square pans (12-ounce capacity)\n6 large eggs for the coating\n12 cups semisweet chocolate\n8 large unsalted butter for decorating\nPreheat oven to 325 degrees F.\nIn a large bowl, combine the flour and sugar. Set aside. In a medium bowl or stand mixer fitted with the paddle attachment, cream the eggs with 1 teaspoon sugar on medium speed until smooth, about 5 minutes. Add the cocoa and continue to cream until fully incorporated, approximately 2 minutes more. Beat in the melted butter and vanilla and mix on low speed for about 1 minute. Fold in 1 large egg. Scrape down the sides of bowl with a rubber spatula and fold in remaining 3 tablespoons of chocolate. On a floured surface, roll the cookie dough into a log about 3 inches thick. Spread the chocolate mixture evenly over the log. Bake for 20 minutes, or until a toothpick inserted into the center comes out clean. Let cool completely on a wire rack. For frosting: In the bowl of an electric mixer, beat the cream cheese with sugar until light and fluffy. When the mixture starts to thicken, add in vanilla. Sift in half of 1 tablespoon of flour for each of your ingredients. Divide the batter between 2 baking trays and top with frosted cookie sheets.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}